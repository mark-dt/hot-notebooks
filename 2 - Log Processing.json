{"version":"5","defaultTimeframe":{"from":"now-1h","to":"now","details":{"from":{"type":"expression","normalized":"now-1h","date":"2023-10-03T13:47:17.717Z"},"to":{"type":"expression","normalized":"now","date":"2023-10-03T14:47:17.717Z"}}},"sections":[{"id":"52d13873-2613-4944-8522-5dd0dabc843c","type":"markdown","markdown":"### Log Processing Lab - 4 Exercises"},{"id":"3b505bdb-309e-4c7c-b511-36493c8d494d","type":"markdown","markdown":"##### Handling Data Masking (10 min)\n\nFor data masking, we have 2 options:\n* Use \"Sensitive Data Masking\" settings. This applies masking on OneAgent side, masked data is not transmitted.\n* Log processing rule - data is transmitted but masked at ingest side. This is your only option if you ingest logs through the API\n\n---\n---\n###### Exercise\n\nIn /var/log/loglab/access.log, we write emails.\nMask them using 2 rules, make sure they don't overlap:\n* Sensitive data masking rule. We just want the email removed, we don't care what the result looks like. \n  * The content of these log lines will be: `Request denied, maskingRuleEmail: customer@dynatrace.com`\n  * You can use this regex: `maskingRuleEmail: [^ ]+\\@[^ ]+\\.[a-zA-Z]+`\n\n* Log processing rule. We want to be more precise here, so I want to see cu***@dy***.com instead of customer@dynatrace.com. \n  * The content of these log lines will be: `Request denied, regularEmail: customer@dynatrace.com`\n  * You can use the following log processing rule:\n```\nUSING(INOUT content)\n| PARSE(content,\"DATA 'regularEmail: ' ALNUM{2}:emailStart LD '@' ALNUM{2}:emailDomain LD '.' LD:emailSuffix \")\n| FIELDS_ADD(content: REPLACE_PATTERN(content, \"'regularEmail: ' LD:email_to_be_masked\", CONCAT(\"regularEmail: \", emailStart, \"***@\",emailDomain,\"***.\",emailSuffix)))\n```\n  * And the following Matcher:\n```\nlog.source == \"/var/log/loglab/access.log\""},{"id":"97076fcb-e03d-4d95-a9d8-e65cd12083b5","type":"dql","showTitle":false,"title":"1. Confirm that customer's email address is now masked, in both ways","state":{"input":{"value":"fetch logs\n| filter log.source == \"/var/log/loglab/access.log\"\n| sort timestamp desc","timeframe":{"from":"now-1h","to":"now","details":{"from":{"type":"expression","normalized":"now-1h","date":"2023-10-03T13:47:17.717Z"},"to":{"type":"expression","normalized":"now","date":"2023-10-03T14:47:17.717Z"}}}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true},"isDavisVisualizationAvailable":true},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","isBackgroundThresholdActive":false},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"0a581765-f43b-4736-ab83-05f224f1ab2a","type":"markdown","markdown":"##### Avoid Ingesting Debug Logs (10 min)\n\nDebug logs are always such a license-devourer, if only we could avoid ingesting them!\n\n---\n---\n\n###### Exercise\nAvoid ingesting debug logs. Put the rule on the same level (host / host group / environment) that you used before. Put the rule above the other(s).\n\nIf you ingested spam.log too, this is your chance to turn that off as well."},{"id":"4c4dae77-882e-4905-a92c-d7fe609034ad","type":"dql","showTitle":false,"title":"2. Ensure that we are no longer capturing any new debug logs","state":{"input":{"value":"fetch logs\n| filter loglevel == \"DEBUG\"\n| sort timestamp desc","timeframe":{"from":"now-2h","to":"now"}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true},"isDavisVisualizationAvailable":true},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"9f43fda8-72da-45d1-a59d-9788c92ee776","type":"markdown","markdown":"##### Set up a parsing rule (10 min)\n\nWe have the following query with a parse. Instead of having to memorize this query, we want to put this into a processing pipeline so the result is accessible for us all the time.\n\n---\n---\n###### Exercise\n\nMove this parse into a log processing rule.\n\nNote that at this point in time we don't preserve types - but try to add the numberOfSteaks field in the pipeline as well. You'll have to use toLong() in the summarize as the current pipeline only outputs strings.\n\nUse the following log matching rule:\n```\nlog.source == \"/var/log/loglab/output.log\" and matchesValue(content,\"*TopLevelElement*\")\n```\n\nUse the following processor definition (The DPL stays the same - the syntax differs elsewhere)\n```\nPARSE(content,\"DATA '<ShoppingList>' KVP{ LD 'Name>' WORD:key '<' LD 'Quantity>' INT:value '<' }:shoppingList\")\n| FIELDS_ADD(numberofsteaks:shoppingList[Steaks])"},{"id":"2fe5f6c8-d891-4c98-af0f-dc62b2b14c5d","type":"dql","showTitle":false,"title":"Parse that we want to put in a processor pipeline","state":{"input":{"value":"fetch logs\n| filter log.source == \"/var/log/loglab/output.log\"\n| filter matchesValue(content,\"*TopLevelElement*\")\n| sort timestamp desc\n| parse content, \"\"\"DATA '<ShoppingList>' KVP{ LD 'Name>' WORD:key '<' LD 'Quantity>' INT:value '<' }:shoppingList\"\"\"\n| fieldsAdd numberOfSteaks = shoppingList[Steaks]\n//| summarize sum(numberOfSteaks)","timeframe":{"from":"now-1h","to":"now","details":{"from":{"type":"expression","normalized":"now-1h","date":"2023-10-03T09:54:05.246Z"},"to":{"type":"expression","normalized":"now","date":"2023-10-03T10:54:05.246Z"}}}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true},"isDavisVisualizationAvailable":true},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","isBackgroundThresholdActive":false},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"6d60786f-8d14-45f1-a38d-a0a275d1dea0","type":"dql","title":"3. Ensure that you now see the new fields after you have created the processing rule","showTitle":false,"state":{"input":{"value":"fetch logs\n| filter log.source == \"/var/log/loglab/output.log\"\n| filter matchesValue(content,\"*TopLevelElement*\")\n| sort timestamp desc","timeframe":{"from":"now-1h","to":"now","details":{"from":{"type":"expression","normalized":"now-1h","date":"2023-10-03T13:47:17.717Z"},"to":{"type":"expression","normalized":"now","date":"2023-10-03T14:47:17.717Z"}}}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true}},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","isBackgroundThresholdActive":false},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"c633fb3a-352e-43cd-8d61-8d8f32400880","type":"markdown","markdown":"##### Security Context (5 min)\n\nSecurity context is important when it comes to log onboarding as this will allow us to assign permissions using record-level permissions. By default we pick up quite a bit (e.g. host group, k8s namespace), but in some scenario's like ingesting data through the API we add no context. So we add a rule to fix this.\n\nWe can either use a field value (so promote a field to security context) or assign a hardcoded value.\n\n--- \n---\n\n**Exercise 1**:\n\nWe ingest logs through the api. Set the security context from API-ingested logs to \"i-like-logs\"\n\nAn example DQL query is included below to identify API-ingested logs\n\n--- \n \n\n**Exercise 2**:\n\nCreate another rule, and assign the value of the host group to the Security Context if the host group is not null.\n\nMatcher:\n`isNotNull(dt.host_group.id)`\n\n---\n\n**Extra**: Put this data in it's own bucket and retrieve it\n\nYou can retrieve data from a bucket by specifying this filter:\n\n`filter dt.system.bucket == \"bucketname\"`"},{"id":"f28ca4f1-fb76-42cb-963a-714c169cb2df","type":"dql","showTitle":false,"title":"4. Check that the logs now have the correct security context","state":{"input":{"value":"fetch logs\n| filter  log.source == \"Api-Ingested\"\n| sort timestamp desc\n//| filter dt.security_context == \"i-like-logs\"\n//| summarize count(), by:{log.source}","timeframe":{"from":"now-2h","to":"now"}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true},"isDavisVisualizationAvailable":true},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","isBackgroundThresholdActive":false},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"e8f9cae3-228e-433c-b569-4e1da2a6f027","type":"markdown","markdown":"##### Create a log metric (10 min)\n\nFor this exercise, we want to turn the steaks from a previous one into a metric that we can track.\n\n---\n**Step 1:** Go to Log Custom Attributes, and create a new attribute called 'numberofsteaks'\n\n---\n**Step 2:** Go to Metrics Extraction, and Add a Log Metric. Give it a name.\n\nUse this matcher:\n```\nlog.source == \"/var/log/loglab/output.log\" and matchesValue(content,\"*TopLevelElement*\")\n```\n\nSet 'Metric Measurement' to Attribute Value. Select numberofsteaks as the attribute. Save the metric."},{"id":"b466df8a-6e4a-4727-8213-1261ae67de25","type":"dql","showTitle":false,"state":{"input":{"value":"timeseries sum(log.steaks)","timeframe":{"from":"now-1h","to":"now","details":{"from":{"type":"expression","normalized":"now-1h","date":"2023-10-03T13:47:17.717Z"},"to":{"type":"expression","normalized":"now","date":"2023-10-03T14:47:17.717Z"}}}},"davis":{"includeLogs":true,"davisVisualization":{"isAvailable":true}},"visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","isBackgroundThresholdActive":false},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"firstVisibleRowIndex":0,"columnWidths":{}}},"state":"idle"}},{"id":"fab3e6fc-50c0-442b-9253-e8e696a8af2a","type":"markdown","markdown":"# Enable All Logs\n\nFor the next exercises, ingest all the logs. Create a rule without conditions, on the same level as before, and place it at the top of the list."}]}
